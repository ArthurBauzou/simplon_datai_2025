{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 group_names  msg_count\n",
      "0                alt.atheism        480\n",
      "1              comp.graphics        584\n",
      "2    comp.os.ms-windows.misc        591\n",
      "3   comp.sys.ibm.pc.hardware        590\n",
      "4      comp.sys.mac.hardware        578\n",
      "5             comp.windows.x        593\n",
      "6               misc.forsale        585\n",
      "7                  rec.autos        594\n",
      "8            rec.motorcycles        598\n",
      "9         rec.sport.baseball        597\n",
      "10          rec.sport.hockey        600\n",
      "11                 sci.crypt        595\n",
      "12           sci.electronics        591\n",
      "13                   sci.med        594\n",
      "14                 sci.space        593\n",
      "15    soc.religion.christian        599\n",
      "16        talk.politics.guns        546\n",
      "17     talk.politics.mideast        564\n",
      "18        talk.politics.misc        465\n",
      "19        talk.religion.misc        377\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "\n",
    "newsgroup = fetch_20newsgroups()\n",
    "\n",
    "# les messages\n",
    "messages = newsgroup['data']\n",
    "\n",
    "# répartition des messages\n",
    "solution = pd.Series(newsgroup['target'])\n",
    "group_df = pd.DataFrame(solution.value_counts().sort_index(), columns=['msg_count'])\n",
    "group_df.insert(0, 'group_names', newsgroup['target_names'])\n",
    "\n",
    "print(group_df)\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui va séparer les données de chaque message\n",
    "def format_message(str) -> dict['headers':dict, 'message':str]: \n",
    "\n",
    "    lines = str.split('\\n')\n",
    "\n",
    "# 1. GET HEADERS\n",
    "    headers = {}\n",
    "    message_index = 0\n",
    "    last_header = ''\n",
    "    for i,l in enumerate(lines) :\n",
    "        # Détetcter le premier saut de ligne qui represente le début du message\n",
    "        if l == '' :\n",
    "            message_index = i\n",
    "            break\n",
    "        \n",
    "        head = l.split(':',1)\n",
    "        # gestion des exceptions\n",
    "        if len(head) < 2 :\n",
    "            # aucune entête\n",
    "            if last_header == '' :\n",
    "                if 'TRASH' not in headers :  headers['TRASH'] = ''\n",
    "                headers['TRASH'] += (head[0])\n",
    "            # entête multi-ligne\n",
    "            else : \n",
    "                headers[last_header] += (head[0])\n",
    "        # cas normal\n",
    "        else :\n",
    "            last_header = head[0]\n",
    "            headers[head[0]] = head[1][1:]\n",
    "\n",
    "# 2. GET MESSAGE\n",
    "    message = '\\n'.join(lines[message_index:])\n",
    "\n",
    "    return {'headers': headers, 'message': message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRASH :  cs.utexas.edu!uunet!olivea!sgigate!sgiblab!adagio.panasonic.com!nntp-server.caltech.edu!keith\n",
      "Subject : Re: <Political Atheists?\n",
      "From : keith@cco.caltech.edu (Keith Allan Schneider) <930401.112329.0u1.rusnews.w165w@mantis.co.uk> <11710@vice.ICO.TEK.COM>\n",
      "Organization : California Institute of Technology, Pasadena\n",
      "NNTP-Posting-Host : lloyd.caltech.edu\n",
      "Lines : 17\n"
     ]
    }
   ],
   "source": [
    "for k,v in format_message(messages[690])['headers'].items():\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "{'Distribution': 2533,\n",
      " 'From': 11314,\n",
      " 'Lines': 11276,\n",
      " 'NNTP-Posting-Host': 2311,\n",
      " 'Nntp-Posting-Host': 2453,\n",
      " 'Organization': 10841,\n",
      " 'Reply-To': 1720,\n",
      " 'Subject': 11314}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Faire la liste de tous les headers pour voir haha\n",
    "headers_list = {}\n",
    "\n",
    "for i, msg in enumerate(messages):\n",
    "    headers = format_message(msg)['headers']\n",
    "    for header in headers.keys():\n",
    "        if header not in headers_list:\n",
    "            headers_list[header] = 1\n",
    "        else:\n",
    "            headers_list[header] += 1\n",
    "\n",
    "print(len(headers_list))\n",
    "\n",
    "# filtrage des entêtes les plus présentes\n",
    "limite = 999\n",
    "main_head = dict(filter(lambda a: True if a[1] > limite else False, headers_list.items()))\n",
    "\n",
    "pprint(main_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un dataframe qui va contenir ces headers pour aider à catégoriser les messages\n",
    "# laisser le texte et la signature en brut\n",
    "\n",
    "# liste des messages\n",
    "mess_list = []\n",
    "# dict des headers, chaque valeur est une liste de la longueur du tableau\n",
    "head_dict = {}\n",
    "for h in main_head.keys():\n",
    "    head_dict[h] = []\n",
    "\n",
    "# remplissage de ces objets\n",
    "for msg in messages:\n",
    "    mf = format_message(msg)\n",
    "    # garniture des messages\n",
    "    mess_list.append(mf['message'][1:])\n",
    "    # garniture des headers\n",
    "    to_fill = list(head_dict.keys()) # réduis à mesure que les valeurs sont renseignées\n",
    "    for header, value in mf['headers'].items():\n",
    "        if header in head_dict:\n",
    "            head_dict[header].append(value)\n",
    "            to_fill.remove(header)\n",
    "    for h in to_fill: head_dict[h].append(None) # remplis les valeurs non renseignées\n",
    "\n",
    "df = pd.DataFrame.from_dict(head_dict)\n",
    "df['Message'] = mess_list\n",
    "\n",
    "# export vers un format de stockage (le csv pose un problème, je ne sais pas quel séparateur utiliser)\n",
    "df.to_json('./newsgroup.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je crois que je comprends ce qu’il faut faire : \n",
    "(c’est faux)\n",
    "1. faire une classification des textes du subset train avec un algo genre kmeans (pas retenu les autres)\\\n",
    "Donc on a pas besoin de faire un trainTestSplit en fait ?\\\n",
    "Ou alors on travaille avec toutes les données et on split nous même, pourquoi pas.\\\n",
    "Mais la grande question c’est si on peut fournir le Y ?\n",
    "On fait de l’apprentissage supervisé ou non ?\n",
    "2. vectoriser chaque groupe obtenu. Idéalement avec les deux methodes : CountVectorizer et TfIDFVectorizer\\\n",
    "Question : Comment les attribuer à un groupe ? \n",
    "3. diviser les textes du groupe de test selon leurs catégories (avec subset='test')\n",
    "4. les vectoriser aussi ?\n",
    "5. comparer les deux (je vois pas comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# copier le code de test de kmeans\n",
    "\n",
    "def bench_k_means(estimator, data, y=None):\n",
    "    estimator.fit(data)\n",
    "    scores = [\n",
    "        \"adjusted_rand_score\",\n",
    "        \"adjusted_mutual_info_score\",\n",
    "        \"silhouette_score\",\n",
    "        \"homogeneity_score\",\n",
    "        \"completeness_score\",\n",
    "        \"v_measure_score\",\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for s in scores:\n",
    "        if s not in [\"silhouette_score\"]:\n",
    "            # with ground-truth\n",
    "            # Yeah !\n",
    "            score = getattr(metrics, s)(y, estimator.labels_)\n",
    "        elif y is not None:\n",
    "            # with NO ground-truth\n",
    "            score = getattr(metrics, s)(data, estimator.labels_)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        results.append(score)\n",
    "        \n",
    "    return pd.DataFrame([results], columns=scores, index=[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20' '25' 'board' 'capability' 'car' 'does' 'don' 'flights' 'info'\n",
      " 'insurance' 'new' 'option' 'parent' 'power' 'scsi' 'ssf' 'tiff' 've'\n",
      " 'year' 'years']\n",
      "[[ 0  0  0  0  4  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  2  0  2  0  1  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  2  1  0  2  0  0  0  0  0 25  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  8  0  0  1  0  0  0  0  0  0  0  0  0  0  0  3  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  1  0  0  0  0  0  8  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  0  1  9  0  1  0  9  0  0  1 10  0 10  0 10  0  0  0  0]\n",
      " [ 2  1  0  0  0  0  1  0  0  0  3  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0  9  0  0  1]\n",
      " [ 1  8  0  0 12  0  2  0  2 16  3  0  0  0  0  0  0  3 17 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# les données\n",
    "Xtrain, Ytrain = fetch_20newsgroups(return_X_y=True, remove=('headers', 'footers', 'quotes'))\n",
    "groups = fetch_20newsgroups()['target_names']\n",
    "\n",
    "# Bien sur les phrases doivent être vectorisées\n",
    "c_vect = CountVectorizer(max_features=20, stop_words='english')\n",
    "X_vec = c_vect.fit_transform(Xtrain[:20])\n",
    "feat_n = c_vect.get_feature_names_out()\n",
    "print(feat_n)\n",
    "print(X_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "0\n",
      "0\n",
      "[  5.18963711 126.3159863   33.30676495 342.98250684 164.35192593\n",
      " 382.10731477 125.24376232 167.02162202 136.74063039 165.57777629\n",
      " 201.22375605 117.15614581 158.21504353 224.17069389 228.08331811\n",
      " 130.01153795 251.64657756 113.75265565 240.42462436 207.46083968]\n",
      "[ 10.51672466 125.8308192   31.76164558 342.52299193 161.43282056\n",
      " 382.2420699  124.7196857  165.99364111 134.98148021 164.43539765\n",
      " 201.45967338 117.55025521 157.87019985 224.18184583 226.98237817\n",
      " 130.56798995 252.91302853 114.95796333 240.59301736 206.71477935]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000, \n",
    "    min_df=0.01, \n",
    "    max_df=0.8, \n",
    "    stop_words='english'\n",
    "    )\n",
    "X_vector = vectorizer.fit_transform(Xtrain)\n",
    "\n",
    "model = KMeans(n_clusters=20, n_init='auto', max_iter=1000)\n",
    "# x_categorized = model.fit_transform(X_vector)\n",
    "model.fit(X_vector)\n",
    "x_trans = model.transform(X_vector)\n",
    "x_categorized = model.predict(X_vector)\n",
    "\n",
    "print(len(x_categorized))\n",
    "print(x_categorized[666])\n",
    "print(x_categorized[888])\n",
    "\n",
    "print(x_trans[666])\n",
    "print(x_trans[888])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster0': ['good',\n",
      "              'use',\n",
      "              'does',\n",
      "              'time',\n",
      "              'think',\n",
      "              'know',\n",
      "              'don',\n",
      "              'just',\n",
      "              'people',\n",
      "              'like'],\n",
      " 'Cluster1': ['ftp',\n",
      "              'version',\n",
      "              'file',\n",
      "              'com',\n",
      "              'image',\n",
      "              'data',\n",
      "              'use',\n",
      "              'window',\n",
      "              'available',\n",
      "              'edu'],\n",
      " 'Cluster10': ['28', '24', 'team', '29', '31', '33', '32', '34', '35', '25'],\n",
      " 'Cluster11': ['80',\n",
      "               '15',\n",
      "               '10',\n",
      "               '20',\n",
      "               '25',\n",
      "               '50',\n",
      "               'appears',\n",
      "               '40',\n",
      "               'dos',\n",
      "               '00'],\n",
      " 'Cluster12': ['said',\n",
      "               'groups',\n",
      "               'don',\n",
      "               'going',\n",
      "               'll',\n",
      "               'options',\n",
      "               'package',\n",
      "               'think',\n",
      "               'president',\n",
      "               'ms'],\n",
      " 'Cluster13': ['input',\n",
      "               'section',\n",
      "               'write',\n",
      "               'build',\n",
      "               'open',\n",
      "               'line',\n",
      "               'return',\n",
      "               'program',\n",
      "               'file',\n",
      "               'output'],\n",
      " 'Cluster14': ['use',\n",
      "               'bit',\n",
      "               'free',\n",
      "               'files',\n",
      "               'version',\n",
      "               'quality',\n",
      "               'format',\n",
      "               'color',\n",
      "               'file',\n",
      "               'image'],\n",
      " 'Cluster15': ['dos',\n",
      "               'available',\n",
      "               'pc',\n",
      "               'ftp',\n",
      "               'edu',\n",
      "               'comments',\n",
      "               'contact',\n",
      "               'type',\n",
      "               'machines',\n",
      "               'version'],\n",
      " 'Cluster16': ['vs',\n",
      "               '1992',\n",
      "               'division',\n",
      "               'games',\n",
      "               'season',\n",
      "               'new',\n",
      "               'team',\n",
      "               'edu',\n",
      "               'league',\n",
      "               'hockey'],\n",
      " 'Cluster17': ['uk',\n",
      "               'yes',\n",
      "               'david',\n",
      "               'john',\n",
      "               'ca',\n",
      "               'comp',\n",
      "               'cs',\n",
      "               'os',\n",
      "               'com',\n",
      "               'edu'],\n",
      " 'Cluster18': ['file',\n",
      "               'image',\n",
      "               'server',\n",
      "               'com',\n",
      "               'ftp',\n",
      "               'send',\n",
      "               'mail',\n",
      "               'pub',\n",
      "               'graphics',\n",
      "               'edu'],\n",
      " 'Cluster19': ['public',\n",
      "               'network',\n",
      "               'mail',\n",
      "               'computer',\n",
      "               'ftp',\n",
      "               'information',\n",
      "               'email',\n",
      "               'electronic',\n",
      "               'internet',\n",
      "               'pub'],\n",
      " 'Cluster2': ['don',\n",
      "              'data',\n",
      "              'information',\n",
      "              'like',\n",
      "              'key',\n",
      "              'new',\n",
      "              'time',\n",
      "              'people',\n",
      "              'use',\n",
      "              'space'],\n",
      " 'Cluster3': ['law',\n",
      "              '1993',\n",
      "              '1992',\n",
      "              'house',\n",
      "              'united',\n",
      "              'states',\n",
      "              'mr',\n",
      "              'control',\n",
      "              'gun',\n",
      "              'file'],\n",
      " 'Cluster4': ['say',\n",
      "              'came',\n",
      "              'like',\n",
      "              'just',\n",
      "              'went',\n",
      "              'don',\n",
      "              'said',\n",
      "              'didn',\n",
      "              'know',\n",
      "              'people'],\n",
      " 'Cluster5': ['17', '18', '11', '20', '15', '13', '12', '16', '10', '14'],\n",
      " 'Cluster6': ['number',\n",
      "              'source',\n",
      "              'file',\n",
      "              'build',\n",
      "              'files',\n",
      "              'use',\n",
      "              'info',\n",
      "              'section',\n",
      "              'rules',\n",
      "              'program'],\n",
      " 'Cluster7': ['package',\n",
      "              'time',\n",
      "              'did',\n",
      "              'said',\n",
      "              'think',\n",
      "              'don',\n",
      "              'going',\n",
      "              'know',\n",
      "              'president',\n",
      "              'mr'],\n",
      " 'Cluster8': ['service',\n",
      "              'address',\n",
      "              'people',\n",
      "              'users',\n",
      "              'information',\n",
      "              'mail',\n",
      "              'use',\n",
      "              'email',\n",
      "              'posting',\n",
      "              'internet'],\n",
      " 'Cluster9': ['united',\n",
      "              'attack',\n",
      "              'world',\n",
      "              'plan',\n",
      "              'time',\n",
      "              'military',\n",
      "              'new',\n",
      "              'secret',\n",
      "              'south',\n",
      "              'war']}\n"
     ]
    }
   ],
   "source": [
    "main_words_by_category = {}\n",
    "\n",
    "for i,cluster in enumerate(model.cluster_centers_):\n",
    "    main_words = cluster.argsort()[-10:]\n",
    "    main_words_by_category[f'Cluster{i}'] = []\n",
    "    for word in main_words:\n",
    "        main_words_by_category[f'Cluster{i}'].append(vectorizer.get_feature_names_out()[word])\n",
    "    \n",
    "pprint(main_words_by_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK.\n",
    "Le vectorizer fait une liste des mots importants\n",
    "comment il fait pour décider de l’importance ça je ne sais pas\n",
    "Probablement que c’est simplement la fréquence des mots, mais cela doit être configurable\n",
    "Puis on peut avoir le compte de chacun de ces mots.\n",
    "\n",
    "Essayons de faire le pseudo code grossier des étapes : \n",
    "\n",
    "Séparation des données\n",
    "vectorisation de chaque categorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = fetch_20newsgroups().target_names\n",
    "posts = {}\n",
    "for cat in categories:\n",
    "    posts[cat] = fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
