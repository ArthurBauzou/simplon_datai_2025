{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 group_names  msg_count\n",
      "0                alt.atheism        480\n",
      "1              comp.graphics        584\n",
      "2    comp.os.ms-windows.misc        591\n",
      "3   comp.sys.ibm.pc.hardware        590\n",
      "4      comp.sys.mac.hardware        578\n",
      "5             comp.windows.x        593\n",
      "6               misc.forsale        585\n",
      "7                  rec.autos        594\n",
      "8            rec.motorcycles        598\n",
      "9         rec.sport.baseball        597\n",
      "10          rec.sport.hockey        600\n",
      "11                 sci.crypt        595\n",
      "12           sci.electronics        591\n",
      "13                   sci.med        594\n",
      "14                 sci.space        593\n",
      "15    soc.religion.christian        599\n",
      "16        talk.politics.guns        546\n",
      "17     talk.politics.mideast        564\n",
      "18        talk.politics.misc        465\n",
      "19        talk.religion.misc        377\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "\n",
    "newsgroup = fetch_20newsgroups()\n",
    "\n",
    "# les messages\n",
    "messages = newsgroup['data']\n",
    "\n",
    "# répartition des messages\n",
    "solution = pd.Series(newsgroup['target'])\n",
    "group_df = pd.DataFrame(solution.value_counts().sort_index(), columns=['msg_count'])\n",
    "group_df.insert(0, 'group_names', newsgroup['target_names'])\n",
    "\n",
    "print(group_df)\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui va séparer les données de chaque message\n",
    "def format_message(str) -> dict['headers':dict, 'message':str]: \n",
    "\n",
    "    lines = str.split('\\n')\n",
    "\n",
    "# 1. GET HEADERS\n",
    "    headers = {}\n",
    "    message_index = 0\n",
    "    last_header = ''\n",
    "    for i,l in enumerate(lines) :\n",
    "        # Détetcter le premier saut de ligne qui represente le début du message\n",
    "        if l == '' :\n",
    "            message_index = i\n",
    "            break\n",
    "        \n",
    "        head = l.split(':',1)\n",
    "        # gestion des exceptions\n",
    "        if len(head) < 2 :\n",
    "            # aucune entête\n",
    "            if last_header == '' :\n",
    "                if 'TRASH' not in headers :  headers['TRASH'] = ''\n",
    "                headers['TRASH'] += (head[0])\n",
    "            # entête multi-ligne\n",
    "            else : \n",
    "                headers[last_header] += (head[0])\n",
    "        # cas normal\n",
    "        else :\n",
    "            last_header = head[0]\n",
    "            headers[head[0]] = head[1][1:]\n",
    "\n",
    "# 2. GET MESSAGE\n",
    "    message = '\\n'.join(lines[message_index:])\n",
    "\n",
    "    return {'headers': headers, 'message': message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRASH :  cs.utexas.edu!uunet!olivea!sgigate!sgiblab!adagio.panasonic.com!nntp-server.caltech.edu!keith\n",
      "Subject : Re: <Political Atheists?\n",
      "From : keith@cco.caltech.edu (Keith Allan Schneider) <930401.112329.0u1.rusnews.w165w@mantis.co.uk> <11710@vice.ICO.TEK.COM>\n",
      "Organization : California Institute of Technology, Pasadena\n",
      "NNTP-Posting-Host : lloyd.caltech.edu\n",
      "Lines : 17\n"
     ]
    }
   ],
   "source": [
    "for k,v in format_message(messages[690])['headers'].items():\n",
    "    print(k,':',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "{'Distribution': 2533,\n",
      " 'From': 11314,\n",
      " 'Lines': 11276,\n",
      " 'NNTP-Posting-Host': 2311,\n",
      " 'Nntp-Posting-Host': 2453,\n",
      " 'Organization': 10841,\n",
      " 'Reply-To': 1720,\n",
      " 'Subject': 11314}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Faire la liste de tous les headers pour voir haha\n",
    "headers_list = {}\n",
    "\n",
    "for i, msg in enumerate(messages):\n",
    "    headers = format_message(msg)['headers']\n",
    "    for header in headers.keys():\n",
    "        if header not in headers_list:\n",
    "            headers_list[header] = 1\n",
    "        else:\n",
    "            headers_list[header] += 1\n",
    "\n",
    "print(len(headers_list))\n",
    "\n",
    "# filtrage des entêtes les plus présentes\n",
    "limite = 999\n",
    "main_head = dict(filter(lambda a: True if a[1] > limite else False, headers_list.items()))\n",
    "\n",
    "pprint(main_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un dataframe qui va contenir ces headers pour aider à catégoriser les messages\n",
    "# laisser le texte et la signature en brut\n",
    "\n",
    "# liste des messages\n",
    "mess_list = []\n",
    "# dict des headers, chaque valeur est une liste de la longueur du tableau\n",
    "head_dict = {}\n",
    "for h in main_head.keys():\n",
    "    head_dict[h] = []\n",
    "\n",
    "# remplissage de ces objets\n",
    "for msg in messages:\n",
    "    mf = format_message(msg)\n",
    "    # garniture des messages\n",
    "    mess_list.append(mf['message'][1:])\n",
    "    # garniture des headers\n",
    "    to_fill = list(head_dict.keys()) # réduis à mesure que les valeurs sont renseignées\n",
    "    for header, value in mf['headers'].items():\n",
    "        if header in head_dict:\n",
    "            head_dict[header].append(value)\n",
    "            to_fill.remove(header)\n",
    "    for h in to_fill: head_dict[h].append(None) # remplis les valeurs non renseignées\n",
    "\n",
    "df = pd.DataFrame.from_dict(head_dict)\n",
    "df['Message'] = mess_list\n",
    "\n",
    "# export vers un format de stockage (le csv pose un problème, je ne sais pas quel séparateur utiliser)\n",
    "df.to_json('./newsgroup.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je crois que je comprends ce qu’il faut faire : \n",
    "(c’est faux)\n",
    "1. faire une classification des textes du subset train avec un algo genre kmeans (pas retenu les autres)\\\n",
    "Donc on a pas besoin de faire un trainTestSplit en fait ?\\\n",
    "Ou alors on travaille avec toutes les données et on split nous même, pourquoi pas.\\\n",
    "Mais la grande question c’est si on peut fournir le Y ?\n",
    "On fait de l’apprentissage supervisé ou non ?\n",
    "2. vectoriser chaque groupe obtenu. Idéalement avec les deux methodes : CountVectorizer et TfIDFVectorizer\\\n",
    "Question : Comment les attribuer à un groupe ? \n",
    "3. diviser les textes du groupe de test selon leurs catégories (avec subset='test')\n",
    "4. les vectoriser aussi ?\n",
    "5. comparer les deux (je vois pas comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# copier le code de test de kmeans\n",
    "\n",
    "def bench_k_means(estimator, data, y=None):\n",
    "    estimator.fit(data)\n",
    "    scores = [\n",
    "        \"adjusted_rand_score\",\n",
    "        \"adjusted_mutual_info_score\",\n",
    "        \"silhouette_score\",\n",
    "        \"homogeneity_score\",\n",
    "        \"completeness_score\",\n",
    "        \"v_measure_score\",\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for s in scores:\n",
    "        if s not in [\"silhouette_score\"]:\n",
    "            # with ground-truth\n",
    "            # Yeah !\n",
    "            score = getattr(metrics, s)(y, estimator.labels_)\n",
    "        elif y is not None:\n",
    "            # with NO ground-truth\n",
    "            score = getattr(metrics, s)(data, estimator.labels_)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        results.append(score)\n",
    "        \n",
    "    return pd.DataFrame([results], columns=scores, index=[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "---- (❁´◡`❁) ----\n",
      "['board' 'capability' 'car' 'does' 'don' 'flights' 'info' 'insurance'\n",
      " 'know' 'new' 'option' 'parent' 'power' 'rate' 'scsi' 'ssf' 'tiff' 've'\n",
      " 'year' 'years']\n",
      "['board' 'capability' 'car' 'does' 'don' 'flights' 'info' 'insurance'\n",
      " 'know' 'new' 'option' 'parent' 'power' 'rate' 'scsi' 'ssf' 'tiff' 've'\n",
      " 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "print(len(stops))\n",
    "print('---- (❁´◡`❁) ----')\n",
    "# C’est pire que l’autre stopwords ??\n",
    "\n",
    "# les données\n",
    "Xtrain, Ytrain = fetch_20newsgroups(return_X_y=True, remove=('headers', 'footers', 'quotes'))\n",
    "groups = fetch_20newsgroups()['target_names']\n",
    "\n",
    "# Bien sur les phrases doivent être vectorisées\n",
    "c_vec = CountVectorizer(max_features=20, stop_words='english', token_pattern=r'(?u)\\b[a-zA-Z]\\w+\\b')\n",
    "X_vec = c_vec.fit_transform(Xtrain[:20])\n",
    "feat_n = c_vec.get_feature_names_out()\n",
    "print(feat_n)\n",
    "\n",
    "# autre methode de vectorisation\n",
    "t_vec = TfidfVectorizer(max_features=20, stop_words='english', token_pattern=r'(?u)\\b[a-zA-Z]\\w+\\b')\n",
    "X_vec = t_vec.fit_transform(Xtrain[:20])\n",
    "feat_n = c_vec.get_feature_names_out()\n",
    "print(feat_n)\n",
    "# print(X_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    min_df=0.01, \n",
    "    max_df=0.8, \n",
    "    stop_words='english',\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z]\\w+\\b'\n",
    "    )\n",
    "X_vector = vectorizer.fit_transform(Xtrain)\n",
    "\n",
    "model = KMeans(n_clusters=20, n_init='auto')\n",
    "model.fit(X_vector)\n",
    "x_trans = model.transform(X_vector)\n",
    "x_categorized = model.predict(X_vector)\n",
    "\n",
    "print(len(x_categorized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster0': ['time',\n",
      "              'don',\n",
      "              'god',\n",
      "              'know',\n",
      "              'believe',\n",
      "              'just',\n",
      "              'people',\n",
      "              'christian',\n",
      "              'church',\n",
      "              'group'],\n",
      " 'Cluster1': ['right',\n",
      "              'good',\n",
      "              'time',\n",
      "              'really',\n",
      "              'say',\n",
      "              'did',\n",
      "              'like',\n",
      "              'know',\n",
      "              'think',\n",
      "              'don'],\n",
      " 'Cluster10': ['anybody',\n",
      "               'looking',\n",
      "               'info',\n",
      "               'help',\n",
      "               'hi',\n",
      "               'does',\n",
      "               'mail',\n",
      "               'know',\n",
      "               'advance',\n",
      "               'thanks'],\n",
      " 'Cluster11': ['problem',\n",
      "               'pub',\n",
      "               'use',\n",
      "               'windows',\n",
      "               'program',\n",
      "               'ftp',\n",
      "               'format',\n",
      "               'directory',\n",
      "               'files',\n",
      "               'file'],\n",
      " 'Cluster12': ['don',\n",
      "               'mit',\n",
      "               'just',\n",
      "               'ftp',\n",
      "               'mail',\n",
      "               'cs',\n",
      "               'university',\n",
      "               'soon',\n",
      "               'banks',\n",
      "               'edu'],\n",
      " 'Cluster13': ['public',\n",
      "               'use',\n",
      "               'secure',\n",
      "               'algorithm',\n",
      "               'government',\n",
      "               'keys',\n",
      "               'clipper',\n",
      "               'encryption',\n",
      "               'chip',\n",
      "               'key'],\n",
      " 'Cluster14': ['jews',\n",
      "               'gun',\n",
      "               'right',\n",
      "               'think',\n",
      "               'state',\n",
      "               'israeli',\n",
      "               'don',\n",
      "               'israel',\n",
      "               'government',\n",
      "               'people'],\n",
      " 'Cluster15': ['internal',\n",
      "               'tape',\n",
      "               'cd',\n",
      "               'controller',\n",
      "               'floppy',\n",
      "               'disk',\n",
      "               'hard',\n",
      "               'drives',\n",
      "               'scsi',\n",
      "               'drive'],\n",
      " 'Cluster2': ['win',\n",
      "              'league',\n",
      "              'play',\n",
      "              'hockey',\n",
      "              'players',\n",
      "              'season',\n",
      "              'games',\n",
      "              'year',\n",
      "              'team',\n",
      "              'game'],\n",
      " 'Cluster3': ['know',\n",
      "              'used',\n",
      "              'problem',\n",
      "              've',\n",
      "              'good',\n",
      "              'time',\n",
      "              'new',\n",
      "              'like',\n",
      "              'does',\n",
      "              'com'],\n",
      " 'Cluster4': ['need',\n",
      "              'know',\n",
      "              'program',\n",
      "              'nasa',\n",
      "              'used',\n",
      "              'like',\n",
      "              'using',\n",
      "              'data',\n",
      "              'space',\n",
      "              'use'],\n",
      " 'Cluster5': ['right',\n",
      "              'll',\n",
      "              'got',\n",
      "              'good',\n",
      "              've',\n",
      "              'bike',\n",
      "              'know',\n",
      "              'don',\n",
      "              'like',\n",
      "              'just'],\n",
      " 'Cluster6': ['excellent',\n",
      "              'sell',\n",
      "              'asking',\n",
      "              'price',\n",
      "              'new',\n",
      "              'shipping',\n",
      "              'condition',\n",
      "              'offer',\n",
      "              'sale',\n",
      "              'car'],\n",
      " 'Cluster7': ['use',\n",
      "              'problem',\n",
      "              'version',\n",
      "              'ms',\n",
      "              'running',\n",
      "              'using',\n",
      "              'program',\n",
      "              'dos',\n",
      "              'window',\n",
      "              'windows'],\n",
      " 'Cluster8': ['say',\n",
      "              'people',\n",
      "              'christian',\n",
      "              'christians',\n",
      "              'faith',\n",
      "              'christ',\n",
      "              'believe',\n",
      "              'bible',\n",
      "              'jesus',\n",
      "              'god'],\n",
      " 'Cluster9': ['pc',\n",
      "              'does',\n",
      "              'controller',\n",
      "              'driver',\n",
      "              'drivers',\n",
      "              'monitor',\n",
      "              'cards',\n",
      "              'bus',\n",
      "              'video',\n",
      "              'card']}\n"
     ]
    }
   ],
   "source": [
    "main_words_by_category = {}\n",
    "\n",
    "for i,cluster in enumerate(model.cluster_centers_):\n",
    "    main_words = cluster.argsort()[-10:]\n",
    "    main_words_by_category[f'Cluster{i}'] = []\n",
    "    for word in main_words:\n",
    "        main_words_by_category[f'Cluster{i}'].append(vectorizer.get_feature_names_out()[word])\n",
    "    \n",
    "pprint(main_words_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  Target\n",
      "0        0         12\n",
      "         1         12\n",
      "         2          7\n",
      "         3          2\n",
      "         4          2\n",
      "                   ..\n",
      "15       8          7\n",
      "         9          2\n",
      "         12         8\n",
      "         14         1\n",
      "         18         2\n",
      "Length: 255, dtype: int64\n",
      "[480, 584, 591, 590, 578, 593, 585, 594, 598, 597, 600, 595, 591, 594, 593, 599, 546, 564, 465, 377]\n",
      "-- CLUSTER 0 --\n",
      "{0: 12, 1: 12, 2: 7, 3: 2, 4: 2, 5: 10, 7: 7, 8: 8, 9: 1, 10: 3, 11: 7, 12: 4, 13: 14, 14: 3, 15: 76, 16: 9, 17: 6, 18: 4, 19: 17}\n",
      "-- CLUSTER 1 --\n",
      "{0: 133, 1: 42, 2: 17, 3: 23, 4: 44, 5: 23, 6: 14, 7: 80, 8: 151, 9: 75, 10: 55, 11: 68, 12: 63, 13: 175, 14: 100, 15: 101, 16: 122, 17: 64, 18: 103, 19: 91}\n",
      "27% du groupe 0\n",
      "25% du groupe 8\n",
      "29% du groupe 13\n",
      "22% du groupe 16\n",
      "22% du groupe 18\n",
      "24% du groupe 19\n",
      "-- CLUSTER 2 --\n",
      "{3: 1, 4: 1, 5: 1, 6: 8, 8: 3, 9: 233, 10: 317, 12: 1, 14: 3, 17: 1, 18: 1, 19: 1}\n",
      "39% du groupe 9\n",
      "52% du groupe 10\n",
      "-- CLUSTER 3 --\n",
      "{0: 141, 1: 195, 2: 129, 3: 148, 4: 200, 5: 191, 6: 157, 7: 188, 8: 213, 9: 195, 10: 151, 11: 140, 12: 250, 13: 196, 14: 193, 15: 117, 16: 175, 17: 161, 18: 161, 19: 134}\n",
      "29% du groupe 0\n",
      "33% du groupe 1\n",
      "21% du groupe 2\n",
      "25% du groupe 3\n",
      "34% du groupe 4\n",
      "32% du groupe 5\n",
      "26% du groupe 6\n",
      "31% du groupe 7\n",
      "35% du groupe 8\n",
      "32% du groupe 9\n",
      "25% du groupe 10\n",
      "23% du groupe 11\n",
      "42% du groupe 12\n",
      "32% du groupe 13\n",
      "32% du groupe 14\n",
      "32% du groupe 16\n",
      "28% du groupe 17\n",
      "34% du groupe 18\n",
      "35% du groupe 19\n",
      "-- CLUSTER 4 --\n",
      "{0: 3, 1: 60, 2: 37, 3: 43, 4: 65, 5: 77, 6: 27, 7: 18, 8: 12, 9: 8, 10: 2, 11: 36, 12: 112, 13: 20, 14: 175, 15: 6, 16: 14, 17: 5, 18: 11, 19: 4}\n",
      "29% du groupe 14\n",
      "-- CLUSTER 5 --\n",
      "{0: 29, 1: 24, 2: 21, 3: 25, 4: 34, 5: 19, 6: 10, 7: 60, 8: 102, 9: 39, 10: 29, 11: 24, 12: 34, 13: 31, 14: 45, 15: 19, 16: 35, 17: 36, 18: 28, 19: 22}\n",
      "-- CLUSTER 6 --\n",
      "{0: 2, 1: 1, 2: 1, 3: 8, 4: 6, 6: 230, 7: 177, 8: 31, 10: 1, 11: 1, 12: 15, 14: 3, 16: 1, 17: 2, 18: 3, 19: 1}\n",
      "39% du groupe 6\n",
      "29% du groupe 7\n",
      "-- CLUSTER 7 --\n",
      "{1: 36, 2: 182, 3: 29, 4: 1, 5: 96, 6: 19, 7: 2, 8: 2, 11: 4, 12: 3, 13: 1, 14: 1}\n",
      "30% du groupe 2\n",
      "-- CLUSTER 8 --\n",
      "{0: 76, 2: 1, 7: 2, 8: 4, 9: 3, 12: 1, 14: 2, 15: 229, 16: 2, 17: 3, 18: 1, 19: 74}\n",
      "38% du groupe 15\n",
      "-- CLUSTER 9 --\n",
      "{1: 33, 2: 47, 3: 117, 4: 54, 5: 6, 6: 36, 10: 1, 11: 1, 12: 12, 16: 2}\n",
      "-- CLUSTER 10 --\n",
      "{0: 3, 1: 95, 2: 40, 3: 60, 4: 71, 5: 76, 6: 22, 7: 25, 8: 31, 9: 18, 10: 13, 11: 6, 12: 53, 13: 31, 14: 21, 15: 12, 16: 4, 17: 1, 18: 4, 19: 2}\n",
      "-- CLUSTER 11 --\n",
      "{0: 2, 1: 63, 2: 71, 3: 7, 4: 7, 5: 48, 7: 1, 8: 2, 11: 13, 12: 4, 14: 3, 16: 2, 17: 2}\n",
      "-- CLUSTER 12 --\n",
      "{0: 10, 1: 21, 2: 23, 3: 16, 4: 12, 5: 24, 6: 21, 7: 11, 8: 20, 9: 15, 10: 18, 11: 15, 12: 19, 13: 87, 14: 19, 15: 1, 16: 12, 17: 8, 18: 6}\n",
      "-- CLUSTER 13 --\n",
      "{0: 1, 1: 1, 2: 2, 3: 5, 4: 2, 5: 14, 7: 1, 9: 1, 11: 222, 12: 5, 14: 1, 17: 1}\n",
      "37% du groupe 11\n",
      "-- CLUSTER 14 --\n",
      "{0: 68, 1: 1, 2: 3, 3: 3, 5: 6, 6: 6, 7: 14, 8: 12, 9: 7, 10: 10, 11: 58, 12: 7, 13: 39, 14: 23, 15: 38, 16: 168, 17: 274, 18: 141, 19: 31}\n",
      "30% du groupe 16\n",
      "48% du groupe 17\n",
      "30% du groupe 18\n",
      "-- CLUSTER 15 --\n",
      "{2: 10, 3: 103, 4: 79, 5: 2, 6: 35, 7: 8, 8: 7, 9: 2, 12: 8, 14: 1, 18: 2}\n"
     ]
    }
   ],
   "source": [
    "# compter le nombre de vraies catégories dans chaque cluster\n",
    "# print(len(x_categorized), len(Ytrain), len(Xtrain))\n",
    "\n",
    "df_cluster = pd.DataFrame(x_categorized, columns=['Cluster'])\n",
    "df_cluster['Target'] = Ytrain\n",
    "\n",
    "df_cluster = df_cluster.groupby(['Cluster', 'Target']).size()\n",
    "\n",
    "print(df_cluster)\n",
    "\n",
    "target_pop = group_df['msg_count'].to_list()\n",
    "print(target_pop)\n",
    "\n",
    "for i in range(16):\n",
    "    print(f'-- CLUSTER {i} --')\n",
    "    pop_dict = df_cluster[i].to_dict()\n",
    "    print(pop_dict)\n",
    "    for k,v in pop_dict.items():\n",
    "        ratio = v/target_pop[int(k)]\n",
    "        if ratio > 0.2:\n",
    "            msg = f'{int(ratio*100)}% du groupe {k}'\n",
    "            print(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
