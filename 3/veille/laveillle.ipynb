{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VEIL\n",
    "<img src=\"./lil_veil.jpg\" alt=\"simone\"/>\n",
    "\n",
    "### PARTIE 1 : La régression linéaire\n",
    "\n",
    "#### 1. Qu’est-ce ?\n",
    "Une régression linéaire est une méthode qui vise à produire une équation qui colle au mieux à un jeu de données.\\\n",
    "La finalité est de prédire au mieux une valeur **y** à partir d’autres données **X**.\\\n",
    "L’objectif en est la constitution d’un modèle mathématique théorique à partir de nombreuses données expérimentales.\\\n",
    "\n",
    "Il est nécessaire donc de bien comprendre la nature des données, de n’inclure que ce qui sera signifiant pour dans le système que l’on cherche à déterminer.\n",
    "\n",
    "Un exemple simple de régression linéaire est *l’ajustement affine*, qui ne cherche qu’une droite de forme ***f(x)=ax+b*** qui passe au mieux au sein d’un nuage de points.\n",
    "\n",
    "Le terme «regression» vient du fait que cette opération vise à compresser l’information : on cherche à régresser d’un jeu de données complexes et un peu cahotique\n",
    "\n",
    "#### 2. Comment ça marche ?\n",
    "La méthode de régression la plus courante est celle des moindres carrés.\n",
    "\n",
    "Dans le cas d’une droite qui passe au milieu de points, on va tester de nombreuses droites et calculer pour chacunes l’écart total pour chaque point.\n",
    "On va ensuite cherche la valeur d’écart minimum de ces droites.\n",
    "\n",
    "C’est là que la méthode des moindres carrés prend son sens : \n",
    "on va comparer non pas directement la différence entre la valeur prédite par notre droite et la mesure, \n",
    "mais le carré de cette différence. Cela résous le problème des valeurs d’erreur négatives : \n",
    "on cherche maintenant une moyenne du carré des erreurs qui doit tendre vers 0.\n",
    "\n",
    "simple fonction numpy des moindres carrés :\n",
    "\n",
    "    def erreur_observation(observation,prediction):\n",
    "        err = np.mean( (observation-prediction)**2  )\n",
    "        return err\n",
    "\n",
    "<img src=\"./gauss.jpg\" alt=\"gauss\"/>\n",
    "\n",
    "#### 3. Pourquoi s’en sert-on ?\n",
    "La régression linéaire est une méthode très utilisée en statistique et en apprentissage automatique.\n",
    "\n",
    "### PARTIE 2 : La fonction erreur\n",
    "mean square error : MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Linear regression\n",
    "#     - What, how, when\n",
    "#     - sklearn > linear_model > LinearRegression\n",
    "# - Error function and metrics.\n",
    "#     - what, how, when\n",
    "#     - sklearn > metrics > mean_squared_error | r2_score\n",
    "# - Dilemme biais-variance. wtf ?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
